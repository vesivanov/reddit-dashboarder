{"mode":"new","time":"day","days":7,"limit":100,"max_pages":50,"results":[{"subreddit":"TechSEO","meta":{"subscribers":37031,"active_user_count":null,"description":"Welcome to Tech SEO, A SubReddit that is dedicated to the tech nerd side of SEO. ","title":"Tech SEO"},"posts":[{"id":"1nnmbr4","subreddit":"TechSEO","title":"Complete website redesign + domain/hosting transfer","selftext":"Hi, I've been working for a year now in a small music store (4 people working in there/located in France). We have noticed a downward trend in terms of revenues for the last 2 years, not so dramatic but still relevant to take consideration and do something about it fast.\n\nI've been noticed about how our website is hosted: right now, we pay 80‚Ç¨ per month (yes, not typo, I was shocked too when I heard about it) for hosting/domain and \"personalized support\". But this is complete legal scam because the website's design is horrendous like it was made in 2010 era. To give context, it's a showcase site with only 4 pages. No way we keep paying 80‚Ç¨/month again.\n\nWith that said, we decided to redesign the entire thing from scratch through HTML. For domain, we'll probably migrate to Hostinger and for hosting, GitHub Pages seems to be a good option.\n\nI've heard about how website redesign could affect SEO, how bad things could go? Will our PageRank drop significantly with the domain/host transfer &amp; redesign? I've used tools like PageSpeed Insights and my redesign has a better score, but I wanted to hear your opinions/tips about it. I am aware this is probably the worst-case scenario when it comes to SEO.\n\nThe good news is that we have good reviews on Google Maps (4,5/5 stars for ~300 reviews) and the PageRank seems to be good.\n\nTechnical info:\nThe live website = almost static website (use of .php, only for animated images): home page, 3 pages, and form page. We have on average 2000 views per month. Hosted in France.\nMy redesign = Static website: home page, 3 to 4 pages, and form page\n\nThank you for your help üôè","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been working for a year now in a small music store (4 people working in there/located in France). We have noticed a downward trend in terms of revenues for the last 2 years, not so dramatic but still relevant to take consideration and do something about it fast.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been noticed about how our website is hosted: right now, we pay 80‚Ç¨ per month (yes, not typo, I was shocked too when I heard about it) for hosting/domain and &amp;quot;personalized support&amp;quot;. But this is complete legal scam because the website&amp;#39;s design is horrendous like it was made in 2010 era. To give context, it&amp;#39;s a showcase site with only 4 pages. No way we keep paying 80‚Ç¨/month again.&lt;/p&gt;\n\n&lt;p&gt;With that said, we decided to redesign the entire thing from scratch through HTML. For domain, we&amp;#39;ll probably migrate to Hostinger and for hosting, GitHub Pages seems to be a good option.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard about how website redesign could affect SEO, how bad things could go? Will our PageRank drop significantly with the domain/host transfer &amp;amp; redesign? I&amp;#39;ve used tools like PageSpeed Insights and my redesign has a better score, but I wanted to hear your opinions/tips about it. I am aware this is probably the worst-case scenario when it comes to SEO.&lt;/p&gt;\n\n&lt;p&gt;The good news is that we have good reviews on Google Maps (4,5/5 stars for ~300 reviews) and the PageRank seems to be good.&lt;/p&gt;\n\n&lt;p&gt;Technical info:\nThe live website = almost static website (use of .php, only for animated images): home page, 3 pages, and form page. We have on average 2000 views per month. Hosted in France.\nMy redesign = Static website: home page, 3 to 4 pages, and form page&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"TryAgain911","url":"https://www.reddit.com/r/TechSEO/comments/1nnmbr4/complete_website_redesign_domainhosting_transfer/","domain":"self.TechSEO","score":0,"num_comments":1,"created_utc":1758547275,"thumbnail":null},{"id":"1nn9e9r","subreddit":"TechSEO","title":"Why Is Screaming Frog Showing Archive.org URLs in the Directory Tree Graph?","selftext":"https://preview.redd.it/840fr37rbmqf1.png?width=1514&amp;format=png&amp;auto=webp&amp;s=e62db20cf9dfba47861a1dadf2189dbf77aad9b2\n\nI was using SF to crawl a site for a new client, and while reviewing the Directory Tree Graph, it returned several URLs like:\n\n[`https://web.archive.org/web/20070617025509im_/http://example.com/images/ALLSMALLPICS/snood1939_small.jpg`](https://web.archive.org/web/20070617025509im_/http://example.com/images/ALLSMALLPICS/snood1939_small.jpg)\n\nI replaced the real domain name with \"example\" to avoid revealing the actual URL.  \nAny ideas why, should I be worried about it?","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/840fr37rbmqf1.png?width=1514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e62db20cf9dfba47861a1dadf2189dbf77aad9b2\"&gt;https://preview.redd.it/840fr37rbmqf1.png?width=1514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e62db20cf9dfba47861a1dadf2189dbf77aad9b2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was using SF to crawl a site for a new client, and while reviewing the Directory Tree Graph, it returned several URLs like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20070617025509im_/http://example.com/images/ALLSMALLPICS/snood1939_small.jpg\"&gt;&lt;code&gt;https://web.archive.org/web/20070617025509im_/http://example.com/images/ALLSMALLPICS/snood1939_small.jpg&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I replaced the real domain name with &amp;quot;example&amp;quot; to avoid revealing the actual URL.&lt;br/&gt;\nAny ideas why, should I be worried about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"eagerforcash","url":"https://www.reddit.com/r/TechSEO/comments/1nn9e9r/why_is_screaming_frog_showing_archiveorg_urls_in/","domain":"self.TechSEO","score":0,"num_comments":4,"created_utc":1758504198,"thumbnail":"https://b.thumbs.redditmedia.com/p805KDhz5I67IBue9j9Iom-nl2Yy6px4FOWJ1922iuM.jpg"},{"id":"1nlhvqa","subreddit":"TechSEO","title":"Can robots.txt be used to allow AI crawling of structured files like llmst.txt?","selftext":"I've done a bit of research on whether the different AI LLMs respect or recognize structured files like robots.txt, llms.txt, llm-policy, vendor-info.json, and ai-summary.html. There has been discussion about these files in the sub. \n\nThe only file universally recognized or 'respected' is robots.txt. There is mixed messaging whether the llms.txt is respected by ChatGPT. (Depending on who you talk to, or the day of the week, the message seems to change.) Google has flat-out said they won't respect llms.txt. Others LLMs send mixed signals. \n\nI want to experiment with the robots.txt to see if this format will encourage LLMs to read these files. I'm curious to get your take. I fully realize that most LLMs don't even \"look\" for files beyond robots.txt. \n\n\n\n  \n\\# === Explicitly Allow AEO Metadata Files ===  \n  \nAllow: /robots.txt  \nAllow: /llms.txt  \nAllow: /ai-summary.html  \nAllow: /llm-policy.json  \nAllow: /vendor-info.json  \n  \nUser-agent: \\*  \nAllow: /  \n  \n\\# AI Training Data Restrictions  \nUser-agent: GPTBot  \nDisallow: /  \n  \nUser-agent: ClaudeBot  \nDisallow: /  \n  \nUser-agent: Google-Extended  \nDisallow: /  \n  \nUser-agent: MistralBot  \nDisallow: /  \n  \nUser-agent: CohereBot  \nDisallow: /  \n  \nUser-agent: PerplexityBot  \nDisallow: /  \n  \nUser-agent: Meta-ExternalAgent  \nDisallow: /  \n  \nUser-agent: Grok-Bot  \nDisallow: /  \n  \nUser-agent: AmazonBot  \nDisallow: /  \n  \nDisallow: /admin/  \nDisallow: /login/  \nDisallow: /checkout/  \nDisallow: /cart/  \nDisallow: /private/","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve done a bit of research on whether the different AI LLMs respect or recognize structured files like robots.txt, llms.txt, llm-policy, vendor-info.json, and ai-summary.html. There has been discussion about these files in the sub. &lt;/p&gt;\n\n&lt;p&gt;The only file universally recognized or &amp;#39;respected&amp;#39; is robots.txt. There is mixed messaging whether the llms.txt is respected by ChatGPT. (Depending on who you talk to, or the day of the week, the message seems to change.) Google has flat-out said they won&amp;#39;t respect llms.txt. Others LLMs send mixed signals. &lt;/p&gt;\n\n&lt;p&gt;I want to experiment with the robots.txt to see if this format will encourage LLMs to read these files. I&amp;#39;m curious to get your take. I fully realize that most LLMs don&amp;#39;t even &amp;quot;look&amp;quot; for files beyond robots.txt. &lt;/p&gt;\n\n&lt;p&gt;# === Explicitly Allow AEO Metadata Files ===  &lt;/p&gt;\n\n&lt;p&gt;Allow: /robots.txt&lt;br/&gt;\nAllow: /llms.txt&lt;br/&gt;\nAllow: /ai-summary.html&lt;br/&gt;\nAllow: /llm-policy.json&lt;br/&gt;\nAllow: /vendor-info.json  &lt;/p&gt;\n\n&lt;p&gt;User-agent: *&lt;br/&gt;\nAllow: /  &lt;/p&gt;\n\n&lt;p&gt;# AI Training Data Restrictions&lt;br/&gt;\nUser-agent: GPTBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: ClaudeBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: Google-Extended&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: MistralBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: CohereBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: PerplexityBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: Meta-ExternalAgent&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: Grok-Bot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;User-agent: AmazonBot&lt;br/&gt;\nDisallow: /  &lt;/p&gt;\n\n&lt;p&gt;Disallow: /admin/&lt;br/&gt;\nDisallow: /login/&lt;br/&gt;\nDisallow: /checkout/&lt;br/&gt;\nDisallow: /cart/&lt;br/&gt;\nDisallow: /private/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"PipelineMarkerter","url":"https://www.reddit.com/r/TechSEO/comments/1nlhvqa/can_robotstxt_be_used_to_allow_ai_crawling_of/","domain":"self.TechSEO","score":0,"num_comments":9,"created_utc":1758322124,"thumbnail":null},{"id":"1nl2ksn","subreddit":"TechSEO","title":"403 Status Code due to cloudflare","selftext":"Ran site in screaming frog and using Check My Links Chrome extension and returned a 403, which is due to cloudflare challenge page. However in GSC the inspected url is indexed and rendered. I shouldn't worry about this right? ","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ran site in screaming frog and using Check My Links Chrome extension and returned a 403, which is due to cloudflare challenge page. However in GSC the inspected url is indexed and rendered. I shouldn&amp;#39;t worry about this right? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"XahX88","url":"https://www.reddit.com/r/TechSEO/comments/1nl2ksn/403_status_code_due_to_cloudflare/","domain":"self.TechSEO","score":2,"num_comments":8,"created_utc":1758285916,"thumbnail":null},{"id":"1nl0mym","subreddit":"TechSEO","title":"Best way to scale schema markup for thousands of pages (Uniform CMS, GTM, or dev templates)?","selftext":"I‚Äôm working on a project where we need to roll out schema markup across a site with **thousands of pages** (programs, locations, FAQs, etc.). Doing this manually isn‚Äôt realistic, so I‚Äôm exploring the best way to scale it.\n\nA few approaches I‚Äôm considering:\n\n* **Template-based JSON-LD**: Creating schema templates that pull in dynamic fields (title, description, address, etc.) from the CMS and automatically inject the right schema per page type.\n* **Uniform CMS**: Since the site is built in Uniform (headless CMS), I‚Äôm wondering if we can build schema components that use variables/placeholders to pull in content fields dynamically and render JSON-LD only on the respective page.\n* **Google Tag Manager**: Possible to inject JSON-LD dynamically via GTM based on URL rules, but not sure if this scales well or is considered best practice.\n\nThe end goal:\n\n* **Scalable** ‚Üí 1 template should cover 100s of pages.\n* **Dynamic** ‚Üí Schema should update automatically if CMS content changes.\n* **Targeted** ‚Üí Schema should only output on the correct pages (program schema on program pages, FAQ schema on FAQ pages, etc.).\n\nHas anyone here dealt with this at scale?\n\n* What‚Äôs the best practice?\n* Is GTM viable for thousands of pages, or should schema live in the CMS codebase?\n\nWould love to hear how others have handled this, especially with headless CMS setups.","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm working on a project where we need to roll out schema markup across a site with &lt;strong&gt;thousands of pages&lt;/strong&gt; (programs, locations, FAQs, etc.). Doing this manually isn‚Äôt realistic, so I‚Äôm exploring the best way to scale it.&lt;/p&gt;\n\n&lt;p&gt;A few approaches I‚Äôm considering:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Template-based JSON-LD&lt;/strong&gt;: Creating schema templates that pull in dynamic fields (title, description, address, etc.) from the CMS and automatically inject the right schema per page type.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Uniform CMS&lt;/strong&gt;: Since the site is built in Uniform (headless CMS), I‚Äôm wondering if we can build schema components that use variables/placeholders to pull in content fields dynamically and render JSON-LD only on the respective page.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Tag Manager&lt;/strong&gt;: Possible to inject JSON-LD dynamically via GTM based on URL rules, but not sure if this scales well or is considered best practice.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The end goal:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt; ‚Üí 1 template should cover 100s of pages.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; ‚Üí Schema should update automatically if CMS content changes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Targeted&lt;/strong&gt; ‚Üí Schema should only output on the correct pages (program schema on program pages, FAQ schema on FAQ pages, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone here dealt with this at scale?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What‚Äôs the best practice?&lt;/li&gt;\n&lt;li&gt;Is GTM viable for thousands of pages, or should schema live in the CMS codebase?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear how others have handled this, especially with headless CMS setups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"Working_Storm_6170","url":"https://www.reddit.com/r/TechSEO/comments/1nl0mym/best_way_to_scale_schema_markup_for_thousands_of/","domain":"self.TechSEO","score":12,"num_comments":13,"created_utc":1758280201,"thumbnail":null},{"id":"1nkwqni","subreddit":"TechSEO","title":"Virtual Hover Menus vs. Physical Menus For SEO. Any Difference?","selftext":"What do you guys think about hover menus vs hover menus that actually have a real link associated with them for SEO.  \n\nFor instance I prefer to have like a header menu item that has \"Services\". When hovered the links to the individual service pages are contained in the sub menu.  Then at the bottom there is a link to \"All Services\" which goes to the parent /services page.  \n\nI see other people put the /services link as a clickable link in the \"Services\" header menu item that trigger the sub menu hover.  In my opinion this is bad for UX though because people sometimes click those menus to trigger the hover instead of intending to navigate to a new page.  That's why I typically prefer to have the \"Services\" menu item be a # link that only triggers the hover.  \n\nIs this bad for SEO though? Am I losing anything by not putting clickable links there?","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think about hover menus vs hover menus that actually have a real link associated with them for SEO.  &lt;/p&gt;\n\n&lt;p&gt;For instance I prefer to have like a header menu item that has &amp;quot;Services&amp;quot;. When hovered the links to the individual service pages are contained in the sub menu.  Then at the bottom there is a link to &amp;quot;All Services&amp;quot; which goes to the parent /services page.  &lt;/p&gt;\n\n&lt;p&gt;I see other people put the /services link as a clickable link in the &amp;quot;Services&amp;quot; header menu item that trigger the sub menu hover.  In my opinion this is bad for UX though because people sometimes click those menus to trigger the hover instead of intending to navigate to a new page.  That&amp;#39;s why I typically prefer to have the &amp;quot;Services&amp;quot; menu item be a # link that only triggers the hover.  &lt;/p&gt;\n\n&lt;p&gt;Is this bad for SEO though? Am I losing anything by not putting clickable links there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"tnhsaesop","url":"https://www.reddit.com/r/TechSEO/comments/1nkwqni/virtual_hover_menus_vs_physical_menus_for_seo_any/","domain":"self.TechSEO","score":2,"num_comments":6,"created_utc":1758265599,"thumbnail":null},{"id":"1njfby6","subreddit":"TechSEO","title":"Backend sends correct information, but on client-side navigation wrong tags are shown","selftext":"Hi everyone,\n\n**The problem:**\n\nBackend always returns correct tags and content when loading a page directly (server-side response). Googlebot and ‚ÄúFetch as Google‚Äù show the correct tags. \n\nProblem happens only for users navigating inside the site. If you go from *category ‚Üí subcategory* without a full reload, the `&lt;head&gt;` (title, description, canonical etc) still shows values from the parent category.\n\n**Question:**\n\nDoes Google rely only on the server-rendered HTML for meta/canonical/robots, or could the client-side mismatch cause issues? ","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Backend always returns correct tags and content when loading a page directly (server-side response). Googlebot and ‚ÄúFetch as Google‚Äù show the correct tags. &lt;/p&gt;\n\n&lt;p&gt;Problem happens only for users navigating inside the site. If you go from &lt;em&gt;category ‚Üí subcategory&lt;/em&gt; without a full reload, the &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; (title, description, canonical etc) still shows values from the parent category.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Does Google rely only on the server-rendered HTML for meta/canonical/robots, or could the client-side mismatch cause issues? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"Complex_Isopod","url":"https://www.reddit.com/r/TechSEO/comments/1njfby6/backend_sends_correct_information_but_on/","domain":"self.TechSEO","score":2,"num_comments":4,"created_utc":1758120798,"thumbnail":null},{"id":"1nj85di","subreddit":"TechSEO","title":"Does Google AI Overview work the same as LLMs?","selftext":"We know that tokenization is a fundamental step in language models (LLMs). It is the process of breaking down text into smaller subword units, known as tokens, which aid in prediction. Therefore, my question: is the tokenization process part of the Google AI overview or not?","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We know that tokenization is a fundamental step in language models (LLMs). It is the process of breaking down text into smaller subword units, known as tokens, which aid in prediction. Therefore, my question: is the tokenization process part of the Google AI overview or not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"ankushmahajann","url":"https://www.reddit.com/r/TechSEO/comments/1nj85di/does_google_ai_overview_work_the_same_as_llms/","domain":"self.TechSEO","score":2,"num_comments":4,"created_utc":1758100275,"thumbnail":null},{"id":"1nj82s7","subreddit":"TechSEO","title":"How can we stop AI to read our website information","selftext":"Can we stop AI like chat Gpt  to stop read our website information or delete any information derived from our website","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can we stop AI like chat Gpt  to stop read our website information or delete any information derived from our website&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"Longjumping-Fig7661","url":"https://www.reddit.com/r/TechSEO/comments/1nj82s7/how_can_we_stop_ai_to_read_our_website_information/","domain":"self.TechSEO","score":5,"num_comments":26,"created_utc":1758099991,"thumbnail":null},{"id":"1nj5m6z","subreddit":"TechSEO","title":"Can we disallow website without using Robots.txt from any other alternative?","selftext":"I know robots.txt is the usual way to stop search engines from crawling pages. But what if I don‚Äôt want to use it? Are there other ways?","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know robots.txt is the usual way to stop search engines from crawling pages. But what if I don‚Äôt want to use it? Are there other ways?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"chandrasekhar121","url":"https://www.reddit.com/r/TechSEO/comments/1nj5m6z/can_we_disallow_website_without_using_robotstxt/","domain":"self.TechSEO","score":10,"num_comments":24,"created_utc":1758090368,"thumbnail":null},{"id":"1nhpfqq","subreddit":"TechSEO","title":"üö´ Best Way to Suppress Redundant Pages for Crawl Budget ‚Äî &lt;meta noindex&gt; vs. X-Robots-Tag?","selftext":"Hey all,   \n  \nI've been working on a large-scale site  (200K+ pages) and need to suppress redundant pages on scale to improve crawl budget and free up resources for high-value content.\n\n Which approach sends the strongest signal to Googlebot?\n\n**1. Meta robots in¬†&lt;head&gt;**  \n&lt;meta name=\"robots\" content=\"noindex, nofollow\"&gt;\n\n* Googlebot must still fetch and parse the page to see this directive.\n* Links may still be discovered until the page is fully processed.\n\n**2. HTTP header¬†X-Robots-Tag**  \nHTTP/1.1 200 OK  \nX-Robots-Tag: noindex, nofollow\n\n* Directive is seen before parsing, saving crawl resources.\n* Prevents indexing and following links more efficiently.\n* Works for HTML + non-HTML (PDFs, images, etc.).\n\n**Questions for the group:**\n\n* For a site with crawl budget challenges, is¬†X-Robots-Tag: noindex, nofollow¬†the stronger and more efficient choice in practice?\n* Any real-world experiences where switching from¬†&lt;meta&gt;¬†to header-level directives improved crawl efficiency?\n* Do you recommend mixing strategies (e.g., meta tags for specific page templates, headers for bulk suppression)?\n\nüôè Curious to hear how others have handled this at scale. ","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a large-scale site  (200K+ pages) and need to suppress redundant pages on scale to improve crawl budget and free up resources for high-value content.&lt;/p&gt;\n\n&lt;p&gt;Which approach sends the strongest signal to Googlebot?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Meta robots in¬†&amp;lt;head&amp;gt;&lt;/strong&gt;&lt;br/&gt;\n&amp;lt;meta name=&amp;quot;robots&amp;quot; content=&amp;quot;noindex, nofollow&amp;quot;&amp;gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Googlebot must still fetch and parse the page to see this directive.&lt;/li&gt;\n&lt;li&gt;Links may still be discovered until the page is fully processed.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;2. HTTP header¬†X-Robots-Tag&lt;/strong&gt;&lt;br/&gt;\nHTTP/1.1 200 OK&lt;br/&gt;\nX-Robots-Tag: noindex, nofollow&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Directive is seen before parsing, saving crawl resources.&lt;/li&gt;\n&lt;li&gt;Prevents indexing and following links more efficiently.&lt;/li&gt;\n&lt;li&gt;Works for HTML + non-HTML (PDFs, images, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions for the group:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For a site with crawl budget challenges, is¬†X-Robots-Tag: noindex, nofollow¬†the stronger and more efficient choice in practice?&lt;/li&gt;\n&lt;li&gt;Any real-world experiences where switching from¬†&amp;lt;meta&amp;gt;¬†to header-level directives improved crawl efficiency?&lt;/li&gt;\n&lt;li&gt;Do you recommend mixing strategies (e.g., meta tags for specific page templates, headers for bulk suppression)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;üôè Curious to hear how others have handled this at scale. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"nitz___","url":"https://www.reddit.com/r/TechSEO/comments/1nhpfqq/best_way_to_suppress_redundant_pages_for_crawl/","domain":"self.TechSEO","score":0,"num_comments":22,"created_utc":1757951114,"thumbnail":null}],"partial":false}],"fetched_at":1758552730228}